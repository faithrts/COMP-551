{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faithrts/COMP-551/blob/GaryBranch/A3_Draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up"
      ],
      "metadata": {
        "id": "8A1SE2icjh69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VCTCTWgB-NKG"
      },
      "outputs": [],
      "source": [
        "### importing libraries and setting the random seed\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "#from matplotlib import cm\n",
        "\n",
        "import re\n",
        "import cv2\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from keras.datasets import mnist\n",
        "from importlib import reload\n",
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "from enum import Enum\n",
        "from numpy.random import RandomState\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(1234)\n",
        "random_state = RandomState(1234)\n",
        "\n",
        "# a folder to store the saved graphs\n",
        "!mkdir images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling"
      ],
      "metadata": {
        "id": "hKHZWj9vjn1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "MtAHDLEXj2T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading and cleaning data"
      ],
      "metadata": {
        "id": "8rHPqW8GC1E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assumes the last column of the dataframe is the labels\n",
        "def x_y_from_df(df):\n",
        "  x = df.iloc[:, :-1].to_numpy()\n",
        "  y = df.iloc[:, -1].to_numpy()\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "2V6syl07WULR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def center_array(arr):\n",
        "  mean = np.mean(arr)\n",
        "  return_arr = np.array(arr) - mean\n",
        "  # return_arr = [item - mean for item in arr]\n",
        "\n",
        "  return return_arr"
      ],
      "metadata": {
        "id": "vudEuApQoTk7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_array(arr):\n",
        "  std = np.std(arr)\n",
        "  return_arr = np.array(arr) / std\n",
        "  # return_arr = [item / std for item in arr]\n",
        "\n",
        "  return return_arr"
      ],
      "metadata": {
        "id": "uWuzGFZrpHxK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_greyscale_array(arr):\n",
        "  # the min and max pixel greyscale values\n",
        "  min = 0\n",
        "  max = 255\n",
        "\n",
        "  return np.array(arr - min) / max - min"
      ],
      "metadata": {
        "id": "-Qc-lUOE-yB7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processes the df by performing mean subtraction and normalization\n",
        "def preprocess_df(df):\n",
        "  preprocessed_df = df.copy()\n",
        "\n",
        "  for col in preprocessed_df.iloc[:, :-1]:\n",
        "    preprocessed_col = normalize_array(center_array(df[col]))\n",
        "    preprocessed_df[col] = preprocessed_col\n",
        "\n",
        "  return preprocessed_df"
      ],
      "metadata": {
        "id": "9XtP7FZtn6mN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_greyscale_df(df):\n",
        "  preprocessed_df = df.copy()\n",
        "\n",
        "  for col in preprocessed_df.iloc[:, :-1]:\n",
        "    preprocessed_col = normalize_greyscale_array(center_array(df[col]))\n",
        "    preprocessed_df[col] = preprocessed_col\n",
        "\n",
        "  return preprocessed_df"
      ],
      "metadata": {
        "id": "euQ4kSY5_ieV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processes the df by performing mean subtraction\n",
        "def preprocess_no_normalization_df(df):\n",
        "  preprocessed_df = df.copy()\n",
        "\n",
        "  for col in preprocessed_df.iloc[:, :-1]:\n",
        "    preprocessed_col = center_array(df[col])\n",
        "    preprocessed_df[col] = preprocessed_col\n",
        "\n",
        "  return preprocessed_df"
      ],
      "metadata": {
        "id": "MDvEhngMdlR0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting graphs"
      ],
      "metadata": {
        "id": "sx9mADR8DD8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_label_distributions(label_arr, dataset_name, labels = 0):\n",
        "  \n",
        "  # creates the plot\n",
        "  plt.figure(figsize = (10,4))\n",
        "  width = 0.6\n",
        "\n",
        "  unique_labels = list(set(label_arr))\n",
        "  counts = []\n",
        "\n",
        "  for label in unique_labels:\n",
        "    count = label_arr.count(label)\n",
        "    counts.append(count)\n",
        "\n",
        "  if labels == 0:\n",
        "    labels = [str(label) for label in unique_labels]\n",
        "\n",
        "  # the colour codes of the bars\n",
        "  colours = ['#001219', '#005f73', '#0a9396', '#94d2bd', '#e9d8a6',\n",
        "            '#ee9b00', '#ca6702', '#bb3e03', '#ae2012', '#9b2226']\n",
        "\n",
        "  plt.bar(labels, counts, width, color = colours)\n",
        "  plt.title(\"Distribution of Class Labels in \" + dataset_name + \" Dataset\")\n",
        "  plt.xlabel(\"Label\")\n",
        "  plt.ylabel(\"Count\")\n",
        "\n",
        "  for index, value in enumerate(counts):\n",
        "    plt.text(index - 0.24, value + 5, str(round(value, 2)))\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "G-UXPNHRww-f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing"
      ],
      "metadata": {
        "id": "FsaKjKTqjylr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clones the github repo\n",
        "!git clone https://github.com/zalandoresearch/fashion-mnist\n",
        "sys.path.insert(1, 'fashion-mnist/utils')\n",
        "\n",
        "# imports the mnist reader from the repo\n",
        "import mnist_reader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDX1OBJ7jUl8",
        "outputId": "6f94208f-0ccc-44bd-c493-1566a34dc924"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fashion-mnist'...\n",
            "remote: Enumerating objects: 762, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 762 (delta 0), reused 3 (delta 0), pack-reused 758\u001b[K\n",
            "Receiving objects: 100% (762/762), 105.85 MiB | 22.00 MiB/s, done.\n",
            "Resolving deltas: 100% (444/444), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and cleaning data"
      ],
      "metadata": {
        "id": "HQbCrZx0-EgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### loading the data using the github repo's mnist_reader.load_mnist\n",
        "\n",
        "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# training data\n",
        "x_train_temp, y_train = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind = 'train')\n",
        "\n",
        "# testing data\n",
        "x_test_temp, y_test = mnist_reader.load_mnist('fashion-mnist/data/fashion', kind = 't10k')"
      ],
      "metadata": {
        "id": "kZITIsTPkA3M"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### creating dataframes to better understand data\n",
        "\n",
        "# training dataframe\n",
        "train_temp_df = pd.DataFrame(x_train_temp)\n",
        "train_temp_df['LABEL'] = y_train\n",
        "\n",
        "# testing dataframe\n",
        "test_temp_df = pd.DataFrame(x_test_temp)\n",
        "test_temp_df['LABEL'] = y_test"
      ],
      "metadata": {
        "id": "PgtXdMOXkxwK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_temp_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZkQzG8t8l5xp",
        "outputId": "51c2a4e4-5707-4f6b-82f7-02effab4531e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0  1  2  3  4  5  6  7   8   9  ...  775  776  777  778  779  780  781  \\\n",
              "0      0  0  0  0  0  0  0  0   0   0  ...    0    0    0    0    0    0    0   \n",
              "1      0  0  0  0  0  1  0  0   0   0  ...  114  130   76    0    0    0    0   \n",
              "2      0  0  0  0  0  0  0  0   0  22  ...    0    1    0    0    0    0    0   \n",
              "3      0  0  0  0  0  0  0  0  33  96  ...    0    0    0    0    0    0    0   \n",
              "4      0  0  0  0  0  0  0  0   0   0  ...    0    0    0    0    0    0    0   \n",
              "...   .. .. .. .. .. .. .. ..  ..  ..  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "59995  0  0  0  0  0  0  0  0   0   0  ...    0    0    0    0    0    0    0   \n",
              "59996  0  0  0  0  0  0  0  0   0   0  ...    0    0    0    0    0    0    0   \n",
              "59997  0  0  0  0  0  0  0  0   0   5  ...    0    0    0    0    0    0    0   \n",
              "59998  0  0  0  0  0  0  0  0   0   0  ...   54   50    5    0    1    0    0   \n",
              "59999  0  0  0  0  0  0  0  0   0   0  ...    0    0    0    0    0    0    0   \n",
              "\n",
              "       782  783  LABEL  \n",
              "0        0    0      9  \n",
              "1        0    0      0  \n",
              "2        0    0      0  \n",
              "3        0    0      3  \n",
              "4        0    0      0  \n",
              "...    ...  ...    ...  \n",
              "59995    0    0      5  \n",
              "59996    0    0      1  \n",
              "59997    0    0      3  \n",
              "59998    0    0      0  \n",
              "59999    0    0      5  \n",
              "\n",
              "[60000 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ad4cddb-99b7-4ff5-b4ff-df648f420fae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>114</td>\n",
              "      <td>130</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>33</td>\n",
              "      <td>96</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>54</td>\n",
              "      <td>50</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ad4cddb-99b7-4ff5-b4ff-df648f420fae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ad4cddb-99b7-4ff5-b4ff-df648f420fae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ad4cddb-99b7-4ff5-b4ff-df648f420fae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### centering and normalizing the data\n",
        "\n",
        "# pre-processing each dataframe\n",
        "train_df = preprocess_df(train_temp_df)\n",
        "test_df = preprocess_df(test_temp_df)\n",
        "\n",
        "# extracting arrays for x_train, y_train, x_test, y_test\n",
        "x_train, y_train = x_y_from_df(train_df)\n",
        "x_test, y_test = x_y_from_df(test_df)"
      ],
      "metadata": {
        "id": "aCWhRpennWnd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### saving a copy of the un-preprocessed (centered and normalized) data\n",
        "\n",
        "x_train_unprocessed, _ = x_y_from_df(train_temp_df)\n",
        "x_test_unprocessed, _ = x_y_from_df(test_temp_df)"
      ],
      "metadata": {
        "id": "n0vemftld2Ja"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### saving a copy of the data normalized based on the pixel greyscale values\n",
        "\n",
        "# pre-processing each dataframe\n",
        "train_greyscale_df = preprocess_greyscale_df(train_temp_df)\n",
        "test_greyscale_df = preprocess_greyscale_df(test_temp_df)\n",
        "\n",
        "x_train_greyscale, y_train_greyscale = x_y_from_df(train_greyscale_df)\n",
        "x_test_greyscale, y_test_greyscale = x_y_from_df(test_greyscale_df)"
      ],
      "metadata": {
        "id": "7XDsp3n9_zaQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### saving a copy of the data reshaped into images for the convnet\n",
        "\n",
        "# reshapes the arrays into image format (28 x 28 pixels)\n",
        "x_train_convnet = np.reshape(x_train, (60000, 28, 28, 1))\n",
        "x_test_convnet = np.reshape(x_test, (10000, 28, 28, 1))\n",
        "\n",
        "y_train_convnet = tf.keras.utils.to_categorical(y_train)\n",
        "y_test_convnet = tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "ZoLHrbjOIlSC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "_S73o8ans37S",
        "outputId": "2cf94ec7-0cf8-4181-ba48-9d5d37d744ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "0     -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "1     -0.008644 -0.023223 -0.039178 -0.041322 -0.057646  0.100437 -0.098878   \n",
              "2     -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "3     -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "4     -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "59995 -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "59996 -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "59997 -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "59998 -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "59999 -0.008644 -0.023223 -0.039178 -0.041322 -0.057646 -0.071167 -0.098878   \n",
              "\n",
              "              7         8         9  ...       775       776       777  \\\n",
              "0     -0.156653 -0.239080 -0.377827  ... -0.474798 -0.394260 -0.406094   \n",
              "1     -0.156653 -0.239080 -0.377827  ...  1.857395  2.697741  1.324682   \n",
              "2     -0.156653 -0.239080  0.198023  ... -0.474798 -0.370475 -0.406094   \n",
              "3     -0.156653  1.145393  2.134974  ... -0.474798 -0.394260 -0.406094   \n",
              "4     -0.156653 -0.239080 -0.377827  ... -0.474798 -0.394260 -0.406094   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "59995 -0.156653 -0.239080 -0.377827  ... -0.474798 -0.394260 -0.406094   \n",
              "59996 -0.156653 -0.239080 -0.377827  ... -0.474798 -0.394260 -0.406094   \n",
              "59997 -0.156653 -0.239080 -0.246952  ... -0.474798 -0.394260 -0.406094   \n",
              "59998 -0.156653 -0.239080 -0.377827  ...  0.629925  0.794971 -0.292227   \n",
              "59999 -0.156653 -0.239080 -0.377827  ... -0.474798 -0.394260 -0.406094   \n",
              "\n",
              "            778       779       780       781       782       783  LABEL  \n",
              "0     -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      9  \n",
              "1     -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      0  \n",
              "2     -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      0  \n",
              "3     -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      3  \n",
              "4     -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      0  \n",
              "...         ...       ...       ...       ...       ...       ...    ...  \n",
              "59995 -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      5  \n",
              "59996 -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      1  \n",
              "59997 -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      3  \n",
              "59998 -0.441358 -0.374489 -0.288156 -0.156811 -0.089673 -0.034147      0  \n",
              "59999 -0.441358 -0.396626 -0.288156 -0.156811 -0.089673 -0.034147      5  \n",
              "\n",
              "[60000 rows x 785 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb1baad4-174c-4a78-8ca4-8fd408f7e022\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>0.100437</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>1.857395</td>\n",
              "      <td>2.697741</td>\n",
              "      <td>1.324682</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>0.198023</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.370475</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>1.145393</td>\n",
              "      <td>2.134974</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.246952</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>0.629925</td>\n",
              "      <td>0.794971</td>\n",
              "      <td>-0.292227</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.374489</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>-0.008644</td>\n",
              "      <td>-0.023223</td>\n",
              "      <td>-0.039178</td>\n",
              "      <td>-0.041322</td>\n",
              "      <td>-0.057646</td>\n",
              "      <td>-0.071167</td>\n",
              "      <td>-0.098878</td>\n",
              "      <td>-0.156653</td>\n",
              "      <td>-0.239080</td>\n",
              "      <td>-0.377827</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.474798</td>\n",
              "      <td>-0.394260</td>\n",
              "      <td>-0.406094</td>\n",
              "      <td>-0.441358</td>\n",
              "      <td>-0.396626</td>\n",
              "      <td>-0.288156</td>\n",
              "      <td>-0.156811</td>\n",
              "      <td>-0.089673</td>\n",
              "      <td>-0.034147</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>60000 rows × 785 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb1baad4-174c-4a78-8ca4-8fd408f7e022')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb1baad4-174c-4a78-8ca4-8fd408f7e022 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb1baad4-174c-4a78-8ca4-8fd408f7e022');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting label distributions for training data\n",
        "plot_label_distributions(train_df['LABEL'].tolist(), 'MNIST Fashion (train)', labels)"
      ],
      "metadata": {
        "id": "7Z1sA2Iu0MYp",
        "outputId": "e7b77b33-abca-40cd-ea1f-c473afa06c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting label distributions for testing data\n",
        "plot_label_distributions(test_df['LABEL'].tolist(), 'MNIST Fashion (test)', labels)"
      ],
      "metadata": {
        "id": "5uB2ZBXO2E2q",
        "outputId": "9076505f-a033-4682-f494-88688cbf657e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAEWCAYAAADW2rtYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd7hcVb3G8e8LoQcSSggQSgKEfpESEBQFiVTF6BUpFyUBNCKCBQWxAuq9YIWLSBOQgHSUS1WqICgtoVeJ1BSSkAQkCELI7/6x1iQ7JzPnzEmm7HPyfp7nPGfP2mvWXmt2++2118xWRGBmZmZm5bNEuytgZmZmZtU5UDMzMzMrKQdqZmZmZiXlQM3MzMyspByomZmZmZWUAzUzMzOzknKgZqUi6SxJ329QWetKmiVpyfz6Dkmfb0TZubw/ShrZqPK6sdwfS3pV0isL+f4XJH200fVqJEm7SJrQyvc2cttbHHS2HUn6kKRnmrjsv0raulnlF5azpaS/NXs5Zp1xoGYtkw/sb0l6Q9Jrkv4m6XBJc7fDiDg8In5UZ1mdBhsR8VJE9I2I9xpQ9xMk/a5D+XtFxJhFLbub9VgX+AawWUSsUSPPSpJOlfRSDlT/kV+v1sq65rqMknR3q5e7MOrd9qrJFwEh6X0d0q/O6bvk1yfk1/sV8vTJaYPz6wsk/bgw/zBJT+f9ZoqkGyWtmC8UZuW/dyW9U3h9VpU6jpL0XiHPLEmnL0x7uxIRd0XExs0oW9I+wBsR8VB+vcC+uQhlh6QNK68j4lHgtbzMWu+5Q9Lbef38U9I4ScdJWmZhl9ssrVqONZYDNWu1fSJiRWA94GTgW8B5jV6IpD6NLrMk1gWmR8TUajMlLQ3cBmwO7AmsBOwITAe2b1UlF1N/Bw6uvJC0Kumzn9Yh3wzgxEpPb2ck7Qz8D3Bg3m82BS6HuRcKfSOiL3Ax8NPK64g4vEaR9xTy9I2II7vbyBI4HLiohcu7GPhiF3mOzOtnTdKF1AHAjZLU7MpZ7+dAzdoiIl6PiGuB/YGRkraA+XsTJK0m6frc+zZD0l2SlpB0ESlguS73ChwraXC+WjxM0kvA7YW0YtC2gaT785XvNZJWycta4HZZpddO0p7Ad4D98/IeyfPn3krN9fqepBclTZV0oaR+eV6lHiNzL9erkr5b67OR1C+/f1ou73u5/I8CtwBr5XpcUOXtB+fP5lMR8WREzImIqRHxo4i4scqytpd0T/6MJ0s6PQd7KDklt+efkh4rrKe9JT2ZexEmSvpm52u8ajsPkfRULuM5SQucDCV9J39eL0g6qJC+jKSf589zitJty+VqLOdbuY5vSHpG0vAa+Yrb3i6SJkj6Rm7/ZEmHdNGki0nbSCUAOxC4GninQ74/5bTPdlEewHak4OohgIiYERFjIuKNOt5bl87WQ619sPD2rSQ9Kul1SZdLWja/b779SdKmeX95TdITkj5RmHeBpF9LuiHX4T5JG9So69LArsCd+XWtfbOfpPPyepuoNFygMgRiQ0l35jq/KunynP6XvJhHcln759d3AMNVRw9ZRLwZEXcAnyAF6R/LZXe2ny2wXEkr5899mqSZeXrtwucwKq+rNyQ932HfODSvz5mSbpK0Xhfts5JzoGZtFRH3AxOAD1WZ/Y08bwAwkHRAjoj4HPASqXeub0T8tPCenUm9DnvUWOTBwKGkK9/ZwGl11PFPpF6Ny/Py3lcl26j89xFgfaAv0PG20k7AxsBw4AeSNq2xyF8B/XI5O+c6HxIRtwJ7AZNyPUZVee9HgT9FxKyu2pW9B3wdWI10YhkOHJHn7Q58GNgo12c/Us8cpF7QL+ZehC2A2+tcXtFU4OOkXr9DgFMkbVOYv0au1yBgJHCOpMrttJNzvbYCNsx5ftBxATn/kcB2ua57AC/UWb81SO0eBBwG/FrSyp3knwQ8SfrcIK23C6vkC+D7wPGSluqiDvcBe0g6UdIH6wkWFkJn66HqPlh4736kntshwJakfWA+uY3XATcDqwNHARcX1iWkHqgTgZWB8cB/16jrUGBOREyATvfNC0j794bA1qR1Uhmf+qNcl5WBtUn7GxHx4Tz/fbmsSs/lROBd0r5bl4h4CRjLvONazf2sxnKXAH5LuvOwLvAW+XgiaQXScWuvvE1/AHg4zxtBWkf/SVpndwGXdtY+Kz8HalYGk4BVqqS/Swqo1ouId/O4l64eTntCvqp9q8b8iyLi8Yh4k3Sy3E913IKqw0HALyPiuRwkfRs4QPP35p0YEW9FxCPAI8ACAV+uywHAtyPijYh4AfgF8Lk667EqMLneSkfEuIi4NyJm52WdTQoOIX3+KwKbAIqIpyJicmHeZpJWioiZEfFgvcssLPuGiPhHJHeSTp4dA/bvR8S/8/wbSOtLwGjg67mH6Q3SyfqAKot5D1gm13WpiHghIv5RZxXfBX6Yt70bgVl0fbK+EDhY0iZA/4i4p1qm3Js8jXnBQ1URcRfppLsNqf3TJf1yEbbZHXKvTuVvhy7WQ1f74GkRMSkiZpCCsa2qLZN04XJyRLwTEbcD15N6HCuujoj7I2I2qWeyWjkA/YFOexMlDQT2Br6WjwVTgVOYt328SwqA1oqItyOinjGUb+Rld8fc41oX+9kCImJ6RPw+Iv6Vt+//7pB/DrCFpOUiYnJEPJHTDwdOyvvqbNJ+sVWlV816JgdqVgaDSON2OvoZ6er65tzNf1wdZb3cjfkvAkuRrnIX1Vq5vGLZfUi9EBXFb2n+i3Ty6mi1XKeOZQ2qsx7TSSfWukjaKN9WeUXSP0kH9tUA8gn1dODXwFRJ50haKb/106ST4Yv5NtKO9S6zsOy9JN2bb6m9lssrrouZOaCueJH0OQ8AlgfGVQIO0u3EAR2XERHjga8BJ+Q2XCZprTqrOD2f7CpqrbOiP5BuzR1J1+Oovgd8F1i2s0wR8ceI2Id00h9B6rVa2G8v3xsR/Qt/93axHrraB+vZptcCXo6IOYW0jtt0PeUAzCRdPHRmPdI+NLmwfZxN6s0DOBYQcH++DXtoF+WRl/laHfmK5h7XOtvPqpG0vKSzlYY+/BP4C9Bf0pJ5n9ifFJRNzreMNym0/X8L7Z6R21rv8cNKyIGatZWk7UgHkQWuanOP0jciYn3SmI+jNW98Ua2eta563NYpTK9Lurp+FXiTdPKv1GtJ5j/xd1XuJNJBslj2bGBKF+/r6FXmXfEXy5pY5/tvJd0qW6HO/GcCTwNDI2Il0m2TuQOgI+K0iNgW2Ix0q/GYnP5ARIwgnfz+D7iizuUBaYwZ8Hvg58DAiOgP3FhcNrByh3asS/qcXyXdCtq8EHD0izSofgERcUlE7ET6TAP4SXfq2h0R8S/gj8CX6CJQi4hbSEHQEZ3lK+SfExG3kW4zb7GIVQW6Xg9d7IP1mgSso/nHtnVnmy4an6qtYuDRcd98Gfg3sFph+1gpIjYHiIhXIuILEbEW6UsCZ6iTb0LmZS0N1P1zI5LWAbYl3XqELvazKr5B6r19f85fuW1ZWS83RcRupIuyp4HfFNr+xQ7B+HIR4Z8Y6cEcqFlbKP2ExMeBy4DfRcRjVfJ8XGngr4DXSbexKlflU0hjuLrrs5I2k7Q88EPgqkg/3/F3YFlJH8tjar5HumVWMQUY3OFkU3Qp8HVJQyT1Zd64mdk18leV63IF8N9KP8GwHnA0UO/PD1xEOlj/XtImSl9CWFVpUP7eVfKvCPwTmJWvyr9UmSFpO0nvz5/Hm8DbwBxJS0s6SFK/iHg3v39OlbILRWnZ4h/pxLcM6fbfbEl7MW9sV9GJeXkfIo2jujL3zPyGNJZq9byAQZIWGJcoaWNJu+aA5G1SgNdZXRvhO8DO+RZXV75L6uGpStIISQcoDS6XpO1Jt8DubUxVO18PXeyD9bqP1Et2rKSllH6qZB/Svt8tEfEO6WKkeBtwvn0z356/GfhFPs4sIWkDpW/QIukzmjcwfyYp0OvsuLIzcHtE/Lur+uWesJ2Ba4D7SUEvdLKf1VjuiqRt9TWlLzwdX1jGwLxdrEAKSGcV6n8W8G1Jm+e8/SR9ppPlWA/gQM1a7TpJb5CCie8CvyQNYK5mKOmgPAu4BzgjIv6c550EfC938XfnG4cXkQYav0K65fQVSN9CJfVsnEu60n+TNIi64sr8f7qkauOxzs9l/wV4nhQUHNWNehUdlZf/HKmn8ZJcfpfyyeSjpKvsW0gnh/tJt1nuq/KWbwL/RRqD8xvyTz9kK+W0maRbVdNJt8IgjZl7Id+WOZw0Rq+WD5BOOh3/vkIKSmfmOlzb4X2v5HmTSOOWDo+Ip/O8b5F6V+7NdbiV6uPHliF98eDVXN7qpPGDTZPHbNX123ER8VfS+qllJvAF4FnSuvwd8LOIuHiRK5qW/wadr4fO9sF6l/EOKTDbi7QezgAOLqzL7jqb+cdsVts3DyYFoU+S2nUV84YEbAfcJ2kWqa1fjYjn8rwTgDH5uFL5rbuDSAFQZ07Px7UpwKmkXso9C7d7O9vPqi33VGA50ud1L+nWfsUSpIu3SaRbmzuTA7+IuJrUY3xZ3i8eJ33utZZjPYCiy7HZZmZm5SHpr6TfLnuoycvZEjg7Iro9BtOsURyomZmZmZWUb32amZmZlZQDNTMzM7OScqBmZmZmVlK98sHVq622WgwePLjd1TAzMzPr0rhx416NiAV+tBt6aaA2ePBgxo4d2+5qmJmZmXVJ0ou15vnWp5mZmVlJOVBroEMPPZTVV1+dLbaY93SXGTNmsNtuuzF06FB22203Zs6cCUBE8JWvfIUNN9yQLbfckgcfnPcbqmPGjGHo0KEMHTqUMWPGtLwdRW6T29QOva094Da5Te3jNvWMNtUUEb3ub9ttt412uPPOO2PcuHGx+eabz0075phj4qSTToqIiJNOOimOPfbYiIi44YYbYs8994w5c+bEPffcE9tvv31EREyfPj2GDBkS06dPjxkzZsSQIUNixowZrW9M5ja5Te3Q29oT4Ta5TW5TI/W2NgFjo0ZM0/agqhl/7QrUIiKef/75+TacjTbaKCZNmhQREZMmTYqNNtooIiJGjx4dl1xyyQL5Lrnkkhg9evTc9I752sFtcpvaobe1J8Jtcpvax20qd5s6C9R867PJpkyZwpprpkfMrbHGGkyZMgWAiRMnss4668zNt/baazNx4sSa6WXiNrlN7dDb2gNuk9vUPm5Tz2gTeIxaS0lCUrur0VBuU8/Q29rU29oDblNP4Tb1DL2pTU0L1CSdL2mqpMcLaatIukXSs/n/yjldkk6TNF7So5K2KbxnZM7/rKSRzapvswwcOJDJkycDMHnyZFZffXUABg0axMsvvzw334QJExg0aFDN9DJxm9ymduht7QG3yW1qH7epZ7QJmtujdgGwZ4e044DbImIocFt+DbAXMDT/jQbOhBTYAccD7we2B46vBHc9xSc+8Ym53yQZM2YMI0aMmJt+4YUXEhHce++99OvXjzXXXJM99tiDm2++mZkzZzJz5kxuvvlm9thjj3Y2YQFuk9vUDr2tPeA2uU3t4zb1jDYBzf0yATAYeLzw+hlgzTy9JvBMnj4bOLBjPuBA4OxC+nz5av2168sEBxxwQKyxxhrRp0+fGDRoUJx77rnx6quvxq677hobbrhhDB8+PKZPnx4REXPmzIkjjjgi1l9//dhiiy3igQcemFvOeeedFxtssEFssMEGcf7557elLRVuk9vUDr2tPRFuk9vUPm5T+dtEJ18mUJrfHJIGA9dHxBb59WsR0T9PC5gZEf0lXQ+cHBF353m3Ad8CdgGWjYgf5/TvA29FxM+rLGs0qTeOddddd9sXX6z5I78No1XXavoyaonpk5pSrkZ9synl1iMuWGC1LrIVTj+z4WV2x5tHfqnhZf76sdsbXmZ3fPk/dm14mVOfubThZXbH6hsf2PAyZ57b3vExK3++8cf2Bw5fsuFl1mu7s95rSrm37ty+B/R89M7ZDS/z6iGrNrzM7vjU89MbXua5G2zc8DK74/P/eKbpy5A0LiKGVZvXti8T5AiyYUeSiDgnIoZFxLABA6o+LsvMzMysR2l1oDZF0poA+f/UnD4RWKeQb+2cVivdzMzMrNdrdaB2LVD55uZI4JpC+sH52587AK9HxGTgJmB3SSvnLxHsntPMzMzMer2m3ZyXdClpjNlqkiaQvr15MnCFpMOAF4H9cvYbgb2B8cC/gEMAImKGpB8BD+R8P4yIGc2qs5mZmVmZNC1Qi4hao3OHV8kbwJdrlHM+cH4Dq2ZmZmbWI/jJBGZmZmYl5UDNzMzMrKQcqJmZmZmVlAM1MzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZmZlZSTlQMzMzMyspB2pmZmZmJeVAzczMzKykHKiZmZmZlZQDNTMzM7OScqBmZmZmVlIO1MzMzMxKyoGamZmZWUk5UDMzMzMrKQdqZmZmZiXlQM3MzMyspByomZmZmZWUAzUzMzOzknKgZmZmZlZSDtTMzMzMSsqBmpmZmVlJOVAzMzMzKykHamZmZmYl5UDNzMzMrKQcqJmZmZmVlAM1MzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZmZlZSTlQMzMzMyuptgRqkr4u6QlJj0u6VNKykoZIuk/SeEmXS1o6510mvx6f5w9uR53NzMzMWq3lgZqkQcBXgGERsQWwJHAA8BPglIjYEJgJHJbfchgwM6efkvOZmZmZ9XrtuvXZB1hOUh9geWAysCtwVZ4/Bvhknh6RX5PnD5ekFtbVzMzMrC1aHqhFxETg58BLpADtdWAc8FpEzM7ZJgCD8vQg4OX83tk5/6ody5U0WtJYSWOnTZvW3EaYmZmZtUA7bn2uTOolGwKsBawA7Lmo5UbEORExLCKGDRgwYFGLMzMzM2u7dtz6/CjwfERMi4h3gT8AHwT651uhAGsDE/P0RGAdgDy/HzC9tVU2MzMza712BGovATtIWj6PNRsOPAn8Gdg35xkJXJOnr82vyfNvj4hoYX3NzMzM2qIdY9TuI30p4EHgsVyHc4BvAUdLGk8ag3Zefst5wKo5/WjguFbX2czMzKwd+nSdpfEi4njg+A7JzwHbV8n7NvCZVtTLzMzMrEz8ZAIzMzOzknKgZmZmZlZSDtTMzMzMSsqBmpmZmVlJOVAzMzMzKykHamZmZmYl5UDNzMzMrKQcqJmZmZmVlAM1MzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZmZlZSTlQMzMzMyspB2pmZmZmJeVAzczMzKykHKiZmZmZlZQDNTMzM7OScqBmZmZmVlIO1MzMzMxKyoGamZmZWUk5UDMzMzMrKQdqZmZmZiXlQM3MzMyspByomZmZmZWUAzUzMzOzknKgZmZmZlZSDtTMzMzMSsqBmpmZmVlJOVAzMzMzKykHamZmZmYl5UDNzMzMrKQcqJmZmZmVVFsCNUn9JV0l6WlJT0naUdIqkm6R9Gz+v3LOK0mnSRov6VFJ27SjzmZmZmat1q4etf8F/hQRmwDvA54CjgNui4ihwG35NcBewND8Nxo4s/XVNTMzM2u9lgdqkvoBHwbOA4iIdyLiNWAEMCZnGwN8Mk+PAC6M5F6gv6Q1W1xtMzMzs5ZrR4/aEGAa8FtJD0k6V9IKwMCImJzzvAIMzNODgJcL75+Q08zMzMx6tXYEan2AbYAzI2Jr4E3m3eYEICICiO4UKmm0pLGSxk6bNq1hlTUzMzNrl3YEahOACRFxX359FSlwm1K5pZn/T83zJwLrFN6/dk6bT0ScExHDImLYgAEDmlZ5MzMzs1ZpeaAWEa8AL0vaOCcNB54ErgVG5rSRwDV5+lrg4Pztzx2A1wu3SM3MzMx6rT71ZJL0wYj4a1dp3XAUcLGkpYHngENIQeMVkg4DXgT2y3lvBPYGxgP/ynnNzMzMer26AjXgV6Tbk12l1SUiHgaGVZk1vEreAL68MMsxMzMz68k6DdQk7Qh8ABgg6ejCrJWAJZtZMTMzM7PFXVc9aksDfXO+FQvp/wT2bValzMzMzKyLQC0i7gTulHRBRLzYojqZmZmZGfWPUVtG0jnA4OJ7ImLXZlTKzMzMzOoP1K4EzgLOBd5rXnXMzMzMrKLeQG12RPhh6GZmZmYtVO8P3l4n6QhJa0papfLX1JqZmZmZLebq7VGrPDHgmEJaAOs3tjpmZmZmVlFXoBYRQ5pdETMzMzObX72PkDq4WnpEXNjY6piZmZlZRb23PrcrTC9LetTTg4ADNTMzM7MmqffW51HF15L6A5c1pUZmZmZmBtT/rc+O3gQ8bs3MzMysieodo3Yd6VuekB7GvilwRbMqZWZmZmb1j1H7eWF6NvBiRExoQn3MzMzMLKvr1md+OPvTwIrAysA7zayUmZmZmdUZqEnaD7gf+AywH3CfpH2bWTEzMzOzxV29tz6/C2wXEVMBJA0AbgWualbFzMzMzBZ39X7rc4lKkJZN78Z7zczMzGwh1Nuj9idJNwGX5tf7Azc2p0pmZmZmBl0EapI2BAZGxDGS/hPYKc+6B7i42ZUzMzMzW5x11aN2KvBtgIj4A/AHAEn/keft09TamZmZmS3GuhpnNjAiHuuYmNMGN6VGZmZmZgZ0Haj172Teco2siJmZmZnNr6tAbaykL3RMlPR5YFxzqmRmZmZm0PUYta8BV0s6iHmB2TBgaeBTzayYmZmZ2eKu00AtIqYAH5D0EWCLnHxDRNze9JqZmZmZLebq+h21iPgz8Ocm18XMzMzMCvx0ATMzM7OScqBmZmZmVlIO1MzMzMxKyoGamZmZWUk5UDMzMzMrqbYFapKWlPSQpOvz6yGS7pM0XtLlkpbO6cvk1+Pz/MHtqrOZmZlZK7WzR+2rwFOF1z8BTomIDYGZwGE5/TBgZk4/JeczMzMz6/XaEqhJWhv4GHBufi1gV+CqnGUM8Mk8PSK/Js8fnvObmZmZ9Wrt6lE7FTgWmJNfrwq8FhGz8+sJwKA8PQh4GSDPfz3nn4+k0ZLGSho7bdq0ZtbdzMzMrCVaHqhJ+jgwNSIa+lD3iDgnIoZFxLABAwY0smgzMzOztqjrEVIN9kHgE5L2BpYFVgL+F+gvqU/uNVsbmJjzTwTWASZI6gP0A6a3vtpmZmZmrdXyHrWI+HZErB0Rg4EDgNsj4iDSs0T3zdlGAtfk6Wvza/L82yMiWlhlMzMzs7Yo0++ofQs4WtJ40hi083L6ecCqOf1o4Lg21c/MzMyspdpx63OuiLgDuCNPPwdsXyXP28BnWloxMzMzsxIoU4+amZmZmRU4UDMzMzMrKQdqZmZmZiXlQM3MzMyspByomZmZmZWUAzUzMzOzknKgZmZmZlZSDtTMzMzMSsqBmpmZmVlJOVAzMzMzKykHamZmZmYl5UDNzMzMrKQcqJmZmZmVlAM1MzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZmZlZSTlQMzMzMyspB2pmZmZmJeVAzczMzKykHKiZmZmZlZQDNTMzM7OScqBmZmZmVlIO1MzMzMxKyoGamZmZWUk5UDMzMzMrKQdqZmZmZiXlQM3MzMyspByomZmZmZWUAzUzMzOzknKgZmZmZlZSDtTMzMzMSqrlgZqkdST9WdKTkp6Q9NWcvoqkWyQ9m/+vnNMl6TRJ4yU9KmmbVtfZzMzMrB3a0aM2G/hGRGwG7AB8WdJmwHHAbRExFLgtvwbYCxia/0YDZ7a+ymZmZmat1/JALSImR8SDefoN4ClgEDACGJOzjQE+madHABdGci/QX9KaLa62mZmZWcu1dYyapMHA1sB9wMCImJxnvQIMzNODgJcLb5uQ0zqWNVrSWEljp02b1rQ6m5mZmbVK2wI1SX2B3wNfi4h/FudFRADRnfIi4pyIGBYRwwYMGNDAmpqZmZm1R1sCNUlLkYK0iyPiDzl5SuWWZv4/NadPBNYpvH3tnGZmZmbWq7XjW58CzgOeiohfFmZdC4zM0yOBawrpB+dvf+4AvF64RWpmZmbWa/VpwzI/CHwOeEzSwzntO8DJwBWSDgNeBPbL824E9gbGA/8CDmltdc3MzMzao+WBWkTcDajG7OFV8gfw5aZWyszMzKyE/GQCMzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZmZlZSTlQMzMzMyspB2pmZmZmJeVAzczMzKykHKiZmZmZlZQDNTMzM7OScqBmZmZmVlIO1MzMzMxKyoGamZmZWUk5UDMzMzMrKQdqZmZmZiXlQM3MzMyspByomZmZmZWUAzUzMzOzknKgZmZmZlZSDtTMzMzMSsqBmpmZmVlJOVAzMzMzKykHamZmZmYl5UDNzMzMrKQcqJmZmZmVlAM1MzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZmZlZSTlQMzMzMyspB2pmZmZmJeVAzczMzKykHKiZmZmZlZQDNTMzM7OS6jGBmqQ9JT0jabyk49pdHzMzM7Nm6xGBmqQlgV8DewGbAQdK2qy9tTIzMzNrrh4RqAHbA+Mj4rmIeAe4DBjR5jqZmZmZNZUiot116JKkfYE9I+Lz+fXngPdHxJGFPKOB0fnlxsAzLa9o960GvNruSjRQb2sPuE09hdvUM/S2NvW29oDb1C7rRcSAajP6tLomzRIR5wDntLse3SFpbEQMa3c9GqW3tQfcpp7CbeoZelubelt7wG0qo55y63MisE7h9do5zczMzKzX6imB2gPAUElDJC0NHABc2+Y6mZmZmTVVj7j1GRGzJR0J3AQsCZwfEU+0uVqN0KNu1daht7UH3Kaewm3qGXpbm3pbe8BtKp0e8WUCMzMzs8VRT7n1aWZmZrbYcaBmZmZmVlIO1KqQtKqkh/PfK5ImFl4v3cn7Bkt6vMa8H0r6aI15oySt1SHtAEnflbSLpA8sWos6t7DtLStJ7+W6Py7pSknLd5H/DknD8vQLklZrTU0XXaGtT0h6RNI3JPWa/VrSGpIuk/QPSeMk3Shpo26W0V/SEc2qYyfL/W5eL4/mdfT+BpQ5d1tdlDyNVK2dtfYjSZ+o9QjAVhzrCstq+LoplL2LpOsbVV6jFY4Zj0h6sFWfeSf1+aSkkLRJnflrbVuzurncbuXvpJwFzt+N1iO+TNBqETEd2ApA0gnArIj4+SKW+YNq6fnxWKOAx4FJhYdJRb0AAAwlSURBVFl7AacB+wCzgL8tyvK7qFun7ZXUJyJmN2v5HUlaMiLeW4Qi3oqISnsuBg4HftmQyi0CSSKNC53TwGKLbV0duARYCTi+w7Jbug4bIX9eVwNjIuKAnPY+YCDw924U1R84Ajij4ZWsQdKOwMeBbSLi3/nE0uMuerrS3XZGxLVU+ca+pD7ALjT5WJeXVdp106L9tHjM2AM4Cdi5ycvszIHA3fn/8V3kLaNRLHj+bqhec+XdapI2l3R/vjJ5VNLQPGtJSb/JV2s3S1ou579A6QkLlSuCn0h6kLRxDgMuzmUtl09QWwEzSEHG1/O8D+Veu9vzMm+TtG6h/LMkjZX0d0kfX8T2Vcq7D/ippK0k3ZuXe7WklXO+Ym/UapJe6OzzkfTZQvrZOVBF0ixJv5D0CLDjotS9g7uADTte5Uo6XdKoLj6Do5V65R6X9LWcdrKkLxfynCDpm3n6GEkP5PaemNMGS3pG0oWknXmdastqhIiYSno6x5FKRkm6VtLtwG2SVpB0fv78H5I0ItdxgXWV896Qr7ofl7R/s+rdiY8A70bEWYU2PgLcLelnuV6PVeomqW/eJx7M6ZXHzJ0MbJDb97MW1X1N4NWI+Heu96sRMUnSD/I28rikc/K+XtmPfpLXw98lfSinL6fUo/iUpKuB5SoLkHRm3t+fqGxvbVC1nXneUYV1sUmu8yhJp+fp4jHmCjoc61pd53xcPrFKnWvtN4Ml3ZXzV+2ZkrRdfs8GkraVdKdSz/BNktbMee6QdKqkscBXm9jualYCZuZ61Np/kPT9fBy7W9KllWPeopLUF9gJOIz0s1uV9F3y53KVpKclXVzZVwp5lpP0R0lfqFLuAsfiGss/Je8/t0kakNNqnesWSFc6p893/m7E57KAiPBfJ3/ACcA3q6T/CjgoTy9NOoAOBmYDW+X0K4DP5ukLgH3z9AvAsYWy7gCGFV5vA1xYbfnAdcDIPH0o8H+F8v9ECr6HAhOAZRe2vbm864Elc/qjwM55+ofAqR3rTnpMxwudfD6b5vovldPPAA7O0wHs16B1Niv/7wNcA3yJdLV+fSHP6cCoKm14IbdjW+AxYAWgL/AEsHX+u7NQzpOk4Gt30lfAldfB9cCH8zYxB9ihSdvnrCppr5F6nUbl7WCVnP4/he2xP6lXaoUa6+rTwG8KZfZrw773FeCUKumfBm4h/VTPQOAl0sm3D7BSYVscn9fHYODxFte9L/Bw/ozPKOw7qxTyXATsU9gGf5Gn9wZuzdNHk36OCGBL0vFlWLGs/DncAWzZcXtuYztfAI7K00cA5+bpUcDpefoC5j/GnECVY20J6lxrv1mefIwlHXPH5uldcrs+AIwD1gWWIvUUDsh59i+s1zuAM1q4bb6X2/808DqwbU6vtf9sl/MvC6wIPNuo9QQcBJyXp/9WqMsuuW5rk46n9wA7FdbTYOBW8vkjp1eO+1WPxVWWHcw77v2gsF3WOtd1eQ5s1p971BbePcB3JH2L9Iyut3L68xHxcJ4eR9qgqrm8k7L3BP5YY96OpNtbkA70OxXmXRERcyLiWeA5oK57/p24MiLek9QP6B8Rd+b0MaQgpDPVPp/hpADoAUkP59fr5/zvAb9fxPpWLJfLH0s6iZ+3EGXsBFwdEW9GxCzgD8CHIuIhYHVJayndhpsZES+TDg67Aw8BD5I++0ov64sRce+iNWmh3RIRM/L07sBx+bO5g3TgXZfq6+oxYLfcy/OhiHi9DXWvZSfg0oh4LyKmAHeSTiYC/kfSo6SD+CBSINdyeZvZltTDOQ24XKkH9yOS7pP0GLArsHnhbX/I/4vHjQ8Dv8tlPko6WVTsp9Qr/1AuZ7OmNKYTnbQTqrenoytj0YY5dNtC1LnWfrMU8Ju8Lq9k/s9/U1KwsE9EvER6/vQWwC25nO+RgpCKzs4HjfZWRGwVEZuQzjUX5t6qWvvPB4FrIuLtiHiDdLHdKAcCl+Xpy/LrivsjYkKkoSIPM/82dA3w24i4sEqZnR2Li+Yw73P/HbBTrXPdQp4DG8Zj1Ook6VPMu3/++Yi4JHfZfwy4UdIXScHRvwtve4/CrYoO3uxkcbuTeg26q+OP4i3qj+R1VseK2cy7hb7s3AVX/3xEGm/07SrlvN3AA/bcMRgVkor1nK+uC+FKYF9gDebt6AJOioizOyx3MPV9jg0haX3Sdjc1JxWXLeDTEfFMh7c91XFdRcTtkrYh9e78WNJtEfHDZte/gydIn3O9DgIGkK7K31W6Db8o63mR5O35DuCOfDL/IqlXbFhEvKw0HrRYv8qx4z26ODZLGkLq+d4uImZKuoA2tbVKO0fmWfW0p2X7RlE361x1v8nrbwrwPtKx5e3C7Mmk9bE1aeySgCciotawjnZ9DvcojdEbQNrXW7b/SFqFdLHyH5KC1DMcko7JWTqeS4vb0F+BPSVdErlbq1g0VY7FdSjtj8q6R61OEXF1vgrZKiLG5hPicxFxGim633IRin+D1KVMjtz7RBrgP9+87G/Mu5d/EGkMVsVnJC0haQNST1XHE/JCyb0pMwvjRj5H6sWA1A29bZ6ee1Kt8fncBuyrNOgdSatIWq8RdazDi8BmkpaR1J/Um9eZu4BPSlpe0grAp5j3WV9OWgf7koI2SE/NODSPuUDSoEo7WyWPsTiL1IVf7aBzE2ncUGVc1Nb5/wLrSulbTP+KiN8BPyPdjm+124FlJI2uJEjaknRrd39JS+Y2fxi4H+gHTM0nmY8AlW2r4z7UdJI21rxxq5DGnFb2x1fzdlJPEPoX4L9ymVsw7zizEunk/rqkgaQvH7VcjXa+uJDFtWQ9LUSdq+43pO1tcu7x+Rwp0Kh4jXThc5KkXUjrfoDSFxmQtJSkYm9qWyiNw1sSmE7t/eevwD6Sls3b7SKNfy7YF7goItaLiMERsQ7wPFDP+MQfkMbW/brKvHqPxUswbx/8L+DuWue6Ls6BTd9u3aO28PYDPifpXeAV0jiGlRayrAuAsyS9BfyC1O1ccR1wldLAzqPy32/zVcc04JBC3pdIJ6yVgMMjoniFt6hG5jouT+o5rCz358AV+WR6QyH/Ap9PRMyQ9D3gZqWfkHgX+DILf2CvW+7BuII0oP95Urd4Z/kfzL0U9+ekc/NtTyLiCUkrAhMjYnJOu1nSpsA9+Xg+C/gs6UqwmSq3eZci9W5eRO1vuP4IOBV4NH/+z5MOutW25e2An0maQ1pPX2pqK6qIiMg92afm27Jvky4MvkYaZ/QI6Sr42Ih4RekbvtflHpKxpDE4RMR0SX9V+umcP0bEMVUW12h9gV/li4LZpPE+o0kn8MdJn/MDdZRzJml/fwp4inRLjoh4RNJDpDa+TDqZtkOtdi7MyXy+Y11E3NXVGxZSd+tca785A/i9pINJ44Pn6xWLiClKX+r6I2k88b7AaZWL8VxmOx6FWDlmQOp9GpmHuNTafx6QdC3ptvsU0rCIRgyFOBD4SYe03+f0em4FfxU4X9JPI+LYSmInx+KpHd7/JrB9PidNJY0bhNrnulrpFzDv/L1jYRhUw/gRUiUj6VxSUNCtMU05qLg+Iq5qSsXMzGyxJKlvRMzKQcpfgNER8WC767W4cI9ayUTE59tdBzMzs4JzJG1GGrM2xkFaa7lHzczMzKyk/GUCMzMzs5JyoGZmZmZWUg7UzMzMzErKgZqZLbYkzepG3rnPdW1G+WZm1ThQMzMzMyspB2pmZgWS9lF6HudDkm7Nv/xf8T5J90h6VtIXCu85RtIDkh6VdGIbqm1mvZQDNTOz+d0N7BARW5MeFH1sYd6WpOcT7gj8QNJaknYnPfR5e9LjiLaV1LIHNptZ7+YfvDUzm9/awOWS1gSWJj0yqOKa/IiYtyT9mRSc7QTszrzHkvUlBW5/aV2Vzay3cqBmZja/XwG/jIhr8wO1TyjM6/gL4UF6XuJJEXF2a6pnZosT3/o0M5tfP2Binh7ZYd4ISctKWhXYhfRg9ZuAQyX1BZA0SNLqraqsmfVu7lEzs8XZ8pImFF7/ktSDdqWkmcDtwJDC/EeBPwOrAT+KiEnAJEmbAvdIApgFfBaY2vzqm1lv52d9mpmZmZWUb32amZmZlZQDNTMzM7OScqBmZmZmVlIO1MzMzMxKyoGamZmZWUk5UDMzMzMrKQdqZmZmZiX1/2HPM1BHkiYDAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing models"
      ],
      "metadata": {
        "id": "a6lVueablnwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "U_WEPKULlIU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers"
      ],
      "metadata": {
        "id": "-7B0imbhlxBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetLayer:\n",
        "  def __init__(self):\n",
        "    self.gradient = None\n",
        "    self.parameters = None\n",
        "        \n",
        "  def forward(self, x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "6G7qW7K8kaeL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(NeuralNetLayer):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    super().__init__()\n",
        "    self.ni = input_size\n",
        "    self.no = output_size\n",
        "\n",
        "    sigma = 0.1\n",
        "    self.w = sigma * random_state.randn(output_size, input_size)\n",
        "    self.b = sigma * random_state.randn(output_size)\n",
        "    self.cur_input = None\n",
        "    self.parameters = [self.w, self.b]\n",
        "\n",
        "  # x is the data inputted into this layer during predictions;\n",
        "  # unless this is the input layer, x is the original data inputted\n",
        "  # into the model, but transformed by the preceding layers\n",
        "  def forward(self, x):\n",
        "    self.cur_input = x\n",
        "    return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    assert self.cur_input is not None, \"Must call forward before backward\"\n",
        "    #dw = gradient.dot(self.cur_input)\n",
        "    dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
        "    db = gradient\n",
        "    self.gradient = [dw, db]\n",
        "    return gradient.dot(self.w)"
      ],
      "metadata": {
        "id": "fLT6hDvqlnWn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLULayer(NeuralNetLayer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "        \n",
        "  def forward(self, x):\n",
        "    self.gradient = np.where(x > 0, 1.0, 0.0)\n",
        "\n",
        "    activated_x = np.maximum(0, x)\n",
        "    return activated_x\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    assert self.gradient is not None, \"Must call forward before backward\"\n",
        "    return gradient * self.gradient"
      ],
      "metadata": {
        "id": "nh2uqaBzltSL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeakyReLULayer(NeuralNetLayer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "        \n",
        "  def forward(self, x):\n",
        "    self.gradient = np.where(x > 0, 1.0, 0.01)\n",
        "\n",
        "    # activated_x = [[float(element) if element > 0 else float(element) * 0.01 for element in sublist] for sublist in x]\n",
        "    activated_x = np.maximum(0.01 * x, x)\n",
        "    return activated_x\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    assert self.gradient is not None, \"Must call forward before backward\"\n",
        "    return gradient * self.gradient"
      ],
      "metadata": {
        "id": "dXZNK6cj0OpR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TanhLayer(NeuralNetLayer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "        \n",
        "  def forward(self, x):\n",
        "    self.gradient = 1 - (np.tanh(x) ** 2)\n",
        "\n",
        "    activated_x = np.tanh(x)\n",
        "    return activated_x\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    assert self.gradient is not None, \"Must call forward before backward\"\n",
        "    return gradient * self.gradient"
      ],
      "metadata": {
        "id": "_Q3WODEGvX9T"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxOutputLayer(NeuralNetLayer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cur_probs = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    exps = np.exp(x)\n",
        "    probs = exps / np.sum(exps, axis=-1)[:, None]\n",
        "    #print(probs)\n",
        "    self.cur_probs = probs\n",
        "    return probs\n",
        "\n",
        "  def backward(self, target):\n",
        "    assert self.cur_probs is not None, \"Must call forward before backward\"\n",
        "    return self.cur_probs - target"
      ],
      "metadata": {
        "id": "H5KJfNG-l3Vq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "5o7wc6mXO2TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  def __init__(self, n_features, n_output, hidden_layers = 0, hidden_units = [0], activation_func = \"None\"):\n",
        "    layers_list = []\n",
        "\n",
        "    # adds the input layer\n",
        "    if hidden_layers == 0:\n",
        "      layers_list.append(LinearLayer(n_features, n_output))\n",
        "    else:\n",
        "      layers_list.append(LinearLayer(n_features, hidden_units[0]))\n",
        "\n",
        "    # adds the hidden layers\n",
        "    for i in range(hidden_layers):\n",
        "\n",
        "      # adds activation function layers\n",
        "      if activation_func.upper() == \"RELU\":\n",
        "        layers_list.append(ReLULayer())\n",
        "      elif activation_func.upper() == \"TANH\":\n",
        "        layers_list.append(TanhLayer())\n",
        "      elif activation_func.upper() == \"LEAKY RELU\":\n",
        "        layers_list.append(LeakyReLULayer())\n",
        "\n",
        "      # if this is the last hidden layer, it outputs the output size\n",
        "      if i == hidden_layers - 1:\n",
        "        layers_list.append(LinearLayer(hidden_units[i], n_output))\n",
        "        continue\n",
        "          \n",
        "      # if this is not the last hidden layer, it outputs the hidden size\n",
        "      layers_list.append(LinearLayer(hidden_units[i], hidden_units[i + 1]))\n",
        "          \n",
        "    # adds the output layer\n",
        "    layers_list.append(SoftmaxOutputLayer())\n",
        "\n",
        "    self.layers = layers_list\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  def backward(self, target):\n",
        "    for layer in self.layers[::-1]:\n",
        "      target = layer.backward(target)\n",
        "\n",
        "  def fit(self, x_train, y_train, x_test, y_test, opt, lambda_reg = 0, track_accuracy = False):    \n",
        "    # one-hot encodes the labels\n",
        "    labels_count = len(set(y_train))\n",
        "    labels = np.eye(labels_count)[np.array(y_train)]\n",
        "\n",
        "    loss = 0\n",
        "    train_acc = 0\n",
        "    test_acc = 0\n",
        "\n",
        "    prediction_probs = self.forward(x_train)\n",
        "\n",
        "    # back propagates\n",
        "    self.backward(labels)\n",
        "    opt.step()\n",
        "\n",
        "    # calculates the loss\n",
        "    loss = self.evaluate_loss(prediction_probs, labels, lambda_reg)\n",
        "\n",
        "    ### calculates the training and testing accuracy\n",
        "    if track_accuracy:\n",
        "      # calculates the predicted labels using the current model weights\n",
        "      train_pred = self.predict(x_train)\n",
        "      test_pred = self.predict(x_test)\n",
        "\n",
        "      # evaluates the training and testing accuracy\n",
        "      train_acc = evaluate_acc(train_pred, y_train)\n",
        "      test_acc = evaluate_acc(test_pred, y_test)\n",
        "\n",
        "    # returns the CE loss, training accuracy, and testing accuracy\n",
        "    return loss, train_acc, test_acc\n",
        "\n",
        "  def fit_mini_batch(self, x_train, y_train, x_test, y_test, opt, batch_size, lambda_reg = 0, track_acc = False):    \n",
        "    # one-hot encodes the labels\n",
        "    labels_count = len(set(y_train))\n",
        "    labels = np.eye(labels_count)[np.array(y_train)]\n",
        "\n",
        "    x_train_size = len(x_train)\n",
        "\n",
        "    # the number of instances in each batch\n",
        "    batch_count = int(len(x_train) / batch_size)\n",
        "\n",
        "    # the index at which the current batch starts (updated in the for loop)\n",
        "    start_index = 0\n",
        "\n",
        "    loss = 0\n",
        "    train_acc = 0\n",
        "    test_acc = 0\n",
        "\n",
        "    # updating the model weights for each batch of training data\n",
        "    for i in range(batch_count):\n",
        "\n",
        "      # the index at which the current batch ends\n",
        "      end_index = batch_size * (i + 1)\n",
        "      if(i == batch_count - 1):\n",
        "        # if this is the last batch, take all the training data from start_index to -1\n",
        "        end_index = -1\n",
        "\n",
        "      # isolates the current batch of training data and associated labels\n",
        "      batch = x_train[start_index : end_index]\n",
        "      batch_labels = labels[start_index : end_index]\n",
        "\n",
        "      # updates the start index for the next batch (next iteration in the for loop)\n",
        "      start_index = end_index\n",
        "\n",
        "      # calculates the class predictions\n",
        "      prediction_probs = self.forward(batch)\n",
        "\n",
        "      # back propagates\n",
        "      self.backward(batch_labels)\n",
        "      opt.step(batch_size / x_train_size)\n",
        "\n",
        "    # calculates the loss\n",
        "    prediction_probs = self.forward(x_train)\n",
        "    loss = self.evaluate_loss(prediction_probs, labels, lambda_reg)\n",
        "\n",
        "    if track_acc:\n",
        "      # calculates the training and testing accuracy\n",
        "      train_pred = self.predict(x_train)\n",
        "      test_pred = self.predict(x_test)\n",
        "      \n",
        "      train_acc = evaluate_acc(train_pred, y_train)\n",
        "      test_acc = evaluate_acc(test_pred, y_test)\n",
        "\n",
        "    # returns the CE loss, training accuracy, and testing accuracy\n",
        "    return loss, train_acc, test_acc\n",
        "\n",
        "  def fit_epoch_mini_batch(self, x_train, y_train, x_test, y_test, opt, batch_size, epochs, lambda_reg, track_acc):\n",
        "    losses = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    indices = np.arange(0, len(x_train))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      random_state.shuffle(indices)\n",
        "      x_train_shuf = x_train[indices]\n",
        "      y_train_shuf = y_train[indices]\n",
        "\n",
        "      loss, train_acc, test_acc = self.fit_mini_batch(x_train_shuf, y_train_shuf, x_test, y_test, opt, batch_size, lambda_reg, track_acc)\n",
        "\n",
        "      # taking the last loss, training accuracy, and testing accuracy values\n",
        "      losses.append(loss)\n",
        "      train_accs.append(train_acc)\n",
        "      test_accs.append(test_acc)\n",
        "    \n",
        "    return losses, train_accs, test_accs\n",
        "\n",
        "  def evaluate_loss(self, prediction_probs, labels, lambda_reg):\n",
        "    # adds regularization term (0 by default)\n",
        "    reg_term = lambda_reg\n",
        "    if lambda_reg != 0:\n",
        "      weights_sum = 0\n",
        "\n",
        "      # iterates through the layers in the model and adds the weights\n",
        "      for layer in self.layers[::-1]:\n",
        "        # if the layer is a LinearLayer, it has weights\n",
        "        if type(layer).__name__ == \"LinearLayer\":\n",
        "          weights_squared = layer.w ** 2\n",
        "          weights_sum += sum(sum(weights_squared))\n",
        "\n",
        "      reg_term = (lambda_reg * weights_sum)\n",
        "\n",
        "    ### calculates the loss, adds it to the losses list, and back propagates based on the loss\n",
        "    loss = -(labels * np.log(prediction_probs)).sum(axis=-1).mean() + reg_term\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def evaluate_initial_metrics(self, x_train, x_test, y_train, y_test, lambda_reg):\n",
        "    # evaluating the CE loss based on the predicted class probabilities\n",
        "    labels_count = len(set(y_train))\n",
        "    labels = np.eye(labels_count)[np.array(y_train)]\n",
        "    prediction_probs = self.forward(x_train)\n",
        "    loss = self.evaluate_loss(prediction_probs, labels, lambda_reg)\n",
        "\n",
        "    # evaluates the training and testing classification accuracy\n",
        "    train_pred = self.predict(x_train)\n",
        "    test_pred = self.predict(x_test)\n",
        "    train_acc = evaluate_acc(train_pred, y_train)\n",
        "    test_acc = evaluate_acc(test_pred, y_test)\n",
        "\n",
        "    return loss, train_acc, test_acc\n",
        "\n",
        "  def predict(self, x):\n",
        "    pred_prob = self.forward(x)\n",
        "    predictions = np.argmax(pred_prob, axis = -1)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "rnVxRvpFmGO4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def __init__(self, net: MLP):\n",
        "        self.net = net\n",
        "\n",
        "    # batch_coef is the number of instances in the batch / training data size\n",
        "    def step(self, batch_coef):\n",
        "        for layer in self.net.layers[::-1]:\n",
        "            if layer.parameters is not None:\n",
        "                self.update(layer.parameters, layer.gradient, batch_coef)\n",
        "\n",
        "    def update(self, params, gradient):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class GradientDescentOptimizer(Optimizer):\n",
        "    def __init__(self, net: MLP, lr: float):\n",
        "        super().__init__(net)\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, gradient, batch_coef):\n",
        "        for (p, g) in zip(params, gradient):\n",
        "            p -= self.lr * g.mean(axis=0) * batch_coef"
      ],
      "metadata": {
        "id": "E9d9aLMlmQ1o"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running experiments"
      ],
      "metadata": {
        "id": "IlELJSq3nWUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "0rywH03KE_9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_acc(true_labels, target_labels):\n",
        "  # the number of labels\n",
        "  label_count = true_labels.shape[0]\n",
        "\n",
        "  # the accuracy is the number of correctly labelled instances divided\n",
        "  # by the total number of instances\n",
        "  accuracy = np.sum(target_labels == true_labels) / label_count\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "bAo7UKZrX5RR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(losses, hidden_layer_count, activation_func):\n",
        "  x_axis = np.arange(1, len(losses) + 1, dtype = int)\n",
        "\n",
        "  plt.figure(figsize=(8, 4))\n",
        "  plt.plot(x_axis, losses)\n",
        "\n",
        "  # changing the title based on whether there are hidden layers\n",
        "  if hidden_layer_count == 0:\n",
        "    plt.title(\"Cross-entropy training loss for \" + str(hidden_layer_count) + \" hidden layers\")\n",
        "  else:\n",
        "    plt.title(\"Cross-entropy training loss for \" + str(hidden_layer_count) + \" hidden layer(s) with \" + activation_func)\n",
        "    \n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.xticks(x_axis, map(str, x_axis))\n",
        "  plt.ylabel(\"Cross-entropy loss\")"
      ],
      "metadata": {
        "id": "m2_I_GRr8MzE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_test_acc(train_accs, test_accs, hidden_layer_count, activation_func):\n",
        "  x_axis = np.arange(1, len(train_accs) + 1, dtype = int)\n",
        "\n",
        "  plt.figure(figsize=(8, 4))\n",
        "\n",
        "  plt.plot(x_axis, train_accs, label = \"Training accuracy\")\n",
        "  plt.plot(x_axis, test_accs, label = \"Testing accuracy\")\n",
        "\n",
        "  # changing the title based on whether there are hidden layers\n",
        "  if hidden_layer_count == 0:\n",
        "    plt.title(\"Training and testing accuracy for \" + str(hidden_layer_count) + \" hidden layers\")\n",
        "  else:\n",
        "    plt.title(\"Training and testing accuracy for \" + str(hidden_layer_count) + \" hidden layer(s) with \" + activation_func)\n",
        "    \n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.xticks(x_axis, map(str, x_axis))\n",
        "  plt.ylabel(\"Classification accuracy\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "8dJsc80n80sE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_mini_batch(x_train, y_train, x_test, y_test, hidden_layers, hidden_units, activation_func, lr, batch_size, epochs, lambda_reg = 0, track_loss = False, track_acc = False):\n",
        "  n_features = x_train.shape[-1]\n",
        "  n_output = len(set(y_train))\n",
        "\n",
        "  # creates the mlp and optimizer objects\n",
        "  mlp = MLP(n_features, n_output, hidden_layers, hidden_units, activation_func)\n",
        "  opt = GradientDescentOptimizer(mlp, lr)\n",
        "\n",
        "  # saving the initial loss, training accuracy, and testing accuracy\n",
        "  initial_loss, initial_train_acc, initial_test_acc = mlp.evaluate_initial_metrics(x_train, x_test, y_train, y_test, lambda_reg)\n",
        "\n",
        "  # training the model using mini-batch times epochs\n",
        "  losses, train_accs, test_accs = mlp.fit_epoch_mini_batch(x_train, y_train, x_test, y_test, opt, batch_size, epochs, lambda_reg, track_acc)\n",
        "\n",
        "  #losses.insert(0, initial_loss)\n",
        "\n",
        "  # the number of hidden layers\n",
        "  hidden_layer_count = int((len(mlp.layers) - 2) / 2)\n",
        "\n",
        "  # plots the cross-entropy loss\n",
        "  if track_loss:\n",
        "    plot_losses(losses, hidden_layer_count, activation_func)\n",
        "    \n",
        "  # plots the training and testing accuracy\n",
        "  if track_acc:\n",
        "    plot_train_test_acc(train_accs, test_accs, hidden_layer_count, activation_func)\n",
        "\n",
        "  return mlp"
      ],
      "metadata": {
        "id": "YWxAE0Mzp7qP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mlp_acc(mlp: MLP, x_train, y_train, x_test, y_test, silent = True):\n",
        "  train_predictions = mlp.predict(x_train)\n",
        "  test_predictions = mlp.predict(x_test)\n",
        "  \n",
        "  train_acc = evaluate_acc(train_predictions, y_train)\n",
        "  test_acc = evaluate_acc(test_predictions, y_test)\n",
        "  \n",
        "  if not silent:\n",
        "    print(\"The accuracy of the model on the training data: \" + str(round((train_acc * 100), 2)) + \"%\")\n",
        "    print(\"The accuracy of the model on the testing data: \" + str(round((test_acc * 100), 2)) + \"%\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "  return train_acc, test_acc\n",
        "                     "
      ],
      "metadata": {
        "id": "_O6LbWx1tNtx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "uVb_gMZBGdRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline accuracy tests\n",
        "\n"
      ],
      "metadata": {
        "id": "_Zi31RJC-Kmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting training and testing for 0 hidden layers (ReLU)\n",
        "\n",
        "mlp0_test = epoch_mini_batch(x_train, y_train, x_test, y_test, 0, [], \"ReLU\", 0.1, 1000, 20, track_acc = True)\n",
        "acc0_test = evaluate_mlp_acc(mlp0_test, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "V5fD_iA3Nirs",
        "outputId": "49edbc54-7385-4445-b12c-c5d49af2bbbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d33b1c5623de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### plotting training and testing for 0 hidden layers (ReLU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmlp0_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ReLU\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0macc0_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_mlp_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp0_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-e26744a40bc0>\u001b[0m in \u001b[0;36mepoch_mini_batch\u001b[0;34m(x_train, y_train, x_test, y_test, hidden_layers, hidden_units, activation_func, lr, batch_size, epochs, lambda_reg, track_loss, track_acc)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# training the model using mini-batch times epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epoch_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#losses.insert(0, initial_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1742701bd1aa>\u001b[0m in \u001b[0;36mfit_epoch_mini_batch\u001b[0;34m(self, x_train, y_train, x_test, y_test, opt, batch_size, epochs, lambda_reg, track_acc)\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0my_train_shuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_shuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_shuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0;31m# taking the last loss, training accuracy, and testing accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1742701bd1aa>\u001b[0m in \u001b[0;36mfit_mini_batch\u001b[0;34m(self, x_train, y_train, x_test, y_test, opt, batch_size, lambda_reg, track_acc)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0;31m# back propagates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mx_train_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1742701bd1aa>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-e015e1418d38>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_input\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Must call forward before backward\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#dw = gradient.dot(self.cur_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting loss for 0 hidden layers (ReLU)\n",
        "\n",
        "mlp0 = epoch_mini_batch(x_train, y_train, x_test, y_test, 0, [], \"ReLU\", 0.1, 1000, 20, track_loss = True)\n",
        "acc0 = evaluate_mlp_acc(mlp0, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "L81lECrRiXgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting training and testing accuracy for 1 hidden layer (ReLU)\n",
        "\n",
        "mlp1_test = epoch_mini_batch(x_train, y_train, x_test, y_test, 1, [128], \"ReLU\", 0.1, 1000, 20, track_acc = True)\n",
        "acc1_test = evaluate_mlp_acc(mlp1_test, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "TJaxXO0Nlzah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting loss for 1 hidden layer (ReLU)\n",
        "\n",
        "mlp1 = epoch_mini_batch(x_train, y_train, x_test, y_test, 1, [128], \"ReLU\", 0.1, 1000, 20, track_loss = True)\n",
        "acc1 = evaluate_mlp_acc(mlp1, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "VWe3uSE-phLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting training and testing accuracy for 2 hidden layers (ReLU)\n",
        "\n",
        "mlp2_test = epoch_mini_batch(x_train, y_train, x_test, y_test, 2, [128, 128], \"ReLU\", 0.1, 1000, 20, track_acc = True)\n",
        "acc2_test = evaluate_mlp_acc(mlp2_test, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "RwkmkB3Vq6yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plotting loss for 2 hidden layers (ReLU)\n",
        "\n",
        "mlp2 = epoch_mini_batch(x_train, y_train, x_test, y_test, 1, [128], \"ReLU\", 0.1, 1000, 20, track_loss = True)\n",
        "acc2 = evaluate_mlp_acc(mlp2, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "oSRHQHOKry5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing activation functions other than ReLU"
      ],
      "metadata": {
        "id": "Xw646-_-vt2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### training and testing 2 hidden layers with Tanh activation functions\n",
        "\n",
        "mlp2t = epoch_mini_batch(x_train, y_train, x_test, y_test, 2, [128, 128], \"Tanh\", 0.1, 1000, 20, track_loss = True)\n",
        "acc2t = evaluate_mlp_acc(mlp2t, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "dx45R6rav2CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### training and testing 2 hidden layers with leaky ReLU activation functions\n",
        "\n",
        "mlp2l = epoch_mini_batch(x_train, y_train, x_test, y_test, 2, [128, 128], \"Leaky ReLU\", 0.1, 1000, 20, track_loss = True)\n",
        "acc2l = evaluate_mlp_acc(mlp2l, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "KNVMb42qwVBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding L2 regularization"
      ],
      "metadata": {
        "id": "lLEAhgccx8KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### training and testing 2 hidden layers with ReLU activation functions and L2 regularization\n",
        "\n",
        "mlp2reg = epoch_mini_batch(x_train, y_train, x_test, y_test, 2, [128, 128], \"ReLU\", 0.1, 1000, 20, track_loss = True, lambda_reg = 0.001)\n",
        "acc2reg = evaluate_mlp_acc(mlp2reg, x_train, y_train, x_test, y_test, silent = False)"
      ],
      "metadata": {
        "id": "iZ0Qy_T_yANL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a model trained on unnormalized images"
      ],
      "metadata": {
        "id": "AzPgdvMUdN95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### training and testing 2 hidden layers with ReLu activation functions\n",
        "\n",
        "### faith - will fix this\n",
        "\n",
        "mlp2ln = epoch_mini_batch(x_train_unprocessed, y_train, x_test_unprocessed, y_test, 2, [128, 128], \"ReLU\", 0.1, 1000, 5, track_loss = True)\n",
        "acc2ln = evaluate_mlp_acc(mlp2ln, x_train_unprocessed, y_train, x_test_unprocessed, y_test, silent = False)"
      ],
      "metadata": {
        "id": "j9hqFKVAdQ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convnet"
      ],
      "metadata": {
        "id": "Tia8YS6kGkEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temp (Gary make functions for the code below pls)"
      ],
      "metadata": {
        "id": "4Ij67vAqJJsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convnetModel(optimizer, num_filters = 8, filter_size = 3, strides = 1, padding = 'valid', activation = 'relu', epochs = 3, batch_size = 60):\n",
        "  model = models.Sequential()\n",
        "  #we add 2 layers as mentionned in the pdf\n",
        "  #these 2 layers are the conv layers\n",
        "  #input shape is 3d because thats how keras library wants it\n",
        "  model.add(layers.Conv2D(filters = num_filters, kernel_size = filter_size, strides = strides,\n",
        "                         padding = padding, activation = activation, input_shape=(28,28,1)))\n",
        "  model.add(layers.Conv2D(filters = num_filters, kernel_size = filter_size, strides = strides,\n",
        "                         padding = padding, activation = activation))\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  #what is this part?\n",
        "  #the first two layers are the fully connected layer, the number of hidden units is specified in the doc\n",
        "  #the last layer is to check how accurate the prediction is?\n",
        "  model.add(layers.Dense(128, activation = activation))\n",
        "  model.add(layers.Dense(128, activation = activation))\n",
        "  model.add(layers.Dense(10, activation = 'softmax'))\n",
        "  #model.summary()\n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss= 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  ### fitting the model\n",
        "  history = model.fit(\n",
        "    x = x_train_convnet,\n",
        "    y = y_train_convnet, \n",
        "    epochs=epochs,                # number of training epochs\n",
        "    batch_size=batch_size\n",
        "  ) \n",
        "\n",
        "  cnn_train_acc = history.history['accuracy']\n",
        "  cnn_train_loss = history.history['loss']\n",
        "\n",
        "  # Overall accuracy\n",
        "  cnn_test_loss, cnn_test_acc = model.evaluate(x_test_convnet, y_test_convnet)\n",
        "  print(\"Test accuracy:\", cnn_test_acc)\n",
        "\n",
        "  predictions = model.predict(y_test_convnet)\n",
        "  print(np.shape(predictions))\n",
        "  #ndx = find_interesting_test_images(predictions, test_labels)\n",
        "  #plot_interesting_test_results(test_images, test_labels, predictions, class_names, ndx)\n",
        "\n",
        "  \n",
        "  #plot_train_test_acc(cnn_train_acc, cnn_test_acc, 2, \"ReLU\")\n",
        "  #plot_losses(cnn_train_loss, 2, \"ReLU\")\n"
      ],
      "metadata": {
        "id": "cfg2JwYEKTFv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optimizers.SGD(lr=0.1, decay=0, momentum=0, nesterov=False)\n",
        "convnetModel(optimizer, num_filters = 8, filter_size = 3, strides = 1, padding = 'valid', activation = 'relu', epochs = 1, batch_size = 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "e-hP-tDbiw-a",
        "outputId": "fc9b1546-b461-4e38-b7fe-5b42f1b08395"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600/600 [==============================] - 37s 61ms/step - loss: 0.5201 - accuracy: 0.8082\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.3925 - accuracy: 0.8575\n",
            "Test accuracy: 0.8575000166893005\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-23273df7f4ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconvnetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-f23aa74ef14d>\u001b[0m in \u001b[0;36mconvnetModel\u001b[0;34m(optimizer, num_filters, filter_size, strides, padding, activation, epochs, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_test_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_convnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;31m#ndx = find_interesting_test_images(predictions, test_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_9\" is incompatible with the layer: expected shape=(None, 28, 28, 1), found shape=(None, 10)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lLEAhgccx8KV",
        "AzPgdvMUdN95"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}